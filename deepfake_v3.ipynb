{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9146200,
          "sourceType": "datasetVersion",
          "datasetId": 5524489
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "deepfake v3",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohingarg12/Deepfake-detection/blob/main/deepfake_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "sanikatiwarekar_deep_fake_detection_dfd_entire_original_dataset_path = kagglehub.dataset_download('sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "rPvaTkKZ8nNa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install retina-face --quiet\n",
        "!pip install opencv-python-headless --quiet\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:48:54.539151Z",
          "iopub.execute_input": "2025-04-08T07:48:54.539436Z",
          "iopub.status.idle": "2025-04-08T07:49:01.079989Z",
          "shell.execute_reply.started": "2025-04-08T07:48:54.539414Z",
          "shell.execute_reply": "2025-04-08T07:49:01.079077Z"
        },
        "id": "RcAR5gJu8nNe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from retinaface import RetinaFace\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "REAL_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\n",
        "FAKE_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n",
        "SAVE_PATH = \"/kaggle/working/faces\"\n",
        "\n",
        "# Create output folders\n",
        "os.makedirs(f\"{SAVE_PATH}/real\", exist_ok=True)\n",
        "os.makedirs(f\"{SAVE_PATH}/fake\", exist_ok=True)\n",
        "\n",
        "# Settings\n",
        "FRAME_SKIP = 30\n",
        "MAX_FACES = 5\n",
        "\n",
        "def extract_faces(video_path, output_dir, video_name):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_idx = 0\n",
        "    saved = 0\n",
        "\n",
        "    while cap.isOpened() and saved < MAX_FACES:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % FRAME_SKIP == 0:\n",
        "            try:\n",
        "                faces = RetinaFace.extract_faces(frame, align=True)\n",
        "                for face in faces:\n",
        "                    img = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "                    img.save(f\"{output_dir}/{video_name}_{saved}.jpg\")\n",
        "                    saved += 1\n",
        "                    if saved >= MAX_FACES:\n",
        "                        break\n",
        "            except:\n",
        "                pass\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "\n",
        "# Run on real and fake videos with tqdm progress bar\n",
        "for label, path in zip(['real', 'fake'], [REAL_PATH, FAKE_PATH]):\n",
        "    print(f\"\\n📦 Extracting {label.upper()} videos:\")\n",
        "    videos = sorted(os.listdir(path))[:5]  # First 5 for now\n",
        "    for video in tqdm(videos, desc=f\"{label.upper()}\"):\n",
        "        video_path = os.path.join(path, video)\n",
        "        video_name = os.path.splitext(video)[0]\n",
        "        extract_faces(video_path, f\"{SAVE_PATH}/{label}\", video_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:51:30.55428Z",
          "iopub.execute_input": "2025-04-08T07:51:30.554623Z",
          "iopub.status.idle": "2025-04-08T07:52:06.076687Z",
          "shell.execute_reply.started": "2025-04-08T07:51:30.554597Z",
          "shell.execute_reply": "2025-04-08T07:52:06.075914Z"
        },
        "id": "1bgLfbye8nNe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ✅ Paths\n",
        "data_path = \"/kaggle/working/faces\"\n",
        "model_save_path = \"/kaggle/working/resnet_model.pth\"\n",
        "\n",
        "# ✅ Transformations (resize + normalize)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# ✅ Dataset and Dataloader\n",
        "dataset = ImageFolder(data_path, transform=transform)\n",
        "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# ✅ Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ✅ Load pretrained ResNet50\n",
        "model = models.resnet50(weights='IMAGENET1K_V1')\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # Binary output\n",
        "model = model.to(device)\n",
        "\n",
        "# ✅ Loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ✅ Training loop\n",
        "epochs = 5\n",
        "train_loss, train_acc = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    train_loss.append(avg_loss)\n",
        "    train_acc.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# ✅ Save model\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"\\n✅ ResNet model saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:55:55.355536Z",
          "iopub.execute_input": "2025-04-08T07:55:55.355874Z",
          "iopub.status.idle": "2025-04-08T07:56:06.57151Z",
          "shell.execute_reply.started": "2025-04-08T07:55:55.35585Z",
          "shell.execute_reply": "2025-04-08T07:56:06.570674Z"
        },
        "id": "I58jJ_xt8nNf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss, label=\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc, label=\"Accuracy\", color=\"green\")\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:56:39.875395Z",
          "iopub.execute_input": "2025-04-08T07:56:39.876006Z",
          "iopub.status.idle": "2025-04-08T07:56:40.261115Z",
          "shell.execute_reply.started": "2025-04-08T07:56:39.875977Z",
          "shell.execute_reply": "2025-04-08T07:56:40.260171Z"
        },
        "id": "NRX2wdgQ8nNg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels --quiet\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:57:58.783292Z",
          "iopub.execute_input": "2025-04-08T07:57:58.783608Z",
          "iopub.status.idle": "2025-04-08T07:58:04.387396Z",
          "shell.execute_reply.started": "2025-04-08T07:57:58.783584Z",
          "shell.execute_reply": "2025-04-08T07:58:04.386378Z"
        },
        "id": "Exs8FNcC8nNh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths\n",
        "data_path = \"/kaggle/working/faces\"\n",
        "model_save_path = \"/kaggle/working/xception_model.pth\"\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),  # Xception expects 299x299\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Dataset\n",
        "dataset = ImageFolder(data_path, transform=transform)\n",
        "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load pretrained Xception\n",
        "xception = pretrainedmodels.__dict__['xception'](pretrained='imagenet')\n",
        "xception.last_linear = nn.Linear(xception.last_linear.in_features, 2)\n",
        "xception = xception.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(xception.parameters(), lr=1e-4)\n",
        "\n",
        "# Train loop\n",
        "epochs = 5\n",
        "train_loss, train_acc = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    xception.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = xception(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    train_loss.append(total_loss / len(train_loader))\n",
        "    train_acc.append(acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f} | Accuracy: {acc:.2f}%\")\n",
        "\n",
        "# Save model\n",
        "torch.save(xception.state_dict(), model_save_path)\n",
        "print(f\"\\n✅ Xception model saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:58:14.111822Z",
          "iopub.execute_input": "2025-04-08T07:58:14.112175Z",
          "iopub.status.idle": "2025-04-08T07:58:23.820736Z",
          "shell.execute_reply.started": "2025-04-08T07:58:14.112146Z",
          "shell.execute_reply": "2025-04-08T07:58:23.819738Z"
        },
        "id": "ZejjBTFu8nNh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss, label=\"Loss\")\n",
        "plt.title(\"Xception Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc, label=\"Accuracy\", color=\"orange\")\n",
        "plt.title(\"Xception Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T07:58:39.513496Z",
          "iopub.execute_input": "2025-04-08T07:58:39.513819Z",
          "iopub.status.idle": "2025-04-08T07:58:39.85009Z",
          "shell.execute_reply.started": "2025-04-08T07:58:39.513795Z",
          "shell.execute_reply": "2025-04-08T07:58:39.849245Z"
        },
        "id": "ga6ajCbx8nNi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/kaggle/working/features\", exist_ok=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:02:02.43261Z",
          "iopub.execute_input": "2025-04-08T08:02:02.432953Z",
          "iopub.status.idle": "2025-04-08T08:02:02.437293Z",
          "shell.execute_reply.started": "2025-04-08T08:02:02.432923Z",
          "shell.execute_reply": "2025-04-08T08:02:02.43645Z"
        },
        "id": "vpuZ2ZBl8nNj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import pretrainedmodels\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Paths ---\n",
        "data_path = \"/kaggle/working/faces\"\n",
        "resnet_path = \"/kaggle/working/resnet_model.pth\"\n",
        "xception_path = \"/kaggle/working/xception_model.pth\"\n",
        "save_path = \"/kaggle/working/features\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# --- Device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms ---\n",
        "resnet_transform = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "xception_transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# --- Load Datasets ---\n",
        "resnet_dataset = datasets.ImageFolder(data_path, transform=resnet_transform)\n",
        "xception_dataset = datasets.ImageFolder(data_path, transform=xception_transform)\n",
        "\n",
        "resnet_loader = DataLoader(resnet_dataset, batch_size=4, shuffle=False)\n",
        "xception_loader = DataLoader(xception_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# --- Load ResNet with trained weights ---\n",
        "resnet = models.resnet50(weights=None)\n",
        "resnet.fc = nn.Linear(2048, 2)  # match what you trained\n",
        "resnet.load_state_dict(torch.load(resnet_path))\n",
        "resnet.fc = nn.Identity()       # remove classifier for feature extraction\n",
        "resnet = resnet.to(device)\n",
        "resnet.eval()\n",
        "\n",
        "# --- Load XceptionNet with trained weights ---\n",
        "xception = pretrainedmodels.__dict__['xception'](pretrained=None)\n",
        "xception.last_linear = nn.Linear(xception.last_linear.in_features, 2)\n",
        "xception.load_state_dict(torch.load(xception_path))\n",
        "xception.last_linear = nn.Identity()\n",
        "xception = xception.to(device)\n",
        "xception.eval()\n",
        "\n",
        "# --- Extract ResNet Features ---\n",
        "resnet_features = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, lbls in resnet_loader:\n",
        "        images = images.to(device)\n",
        "        feats = resnet(images)\n",
        "        resnet_features.append(feats.cpu())\n",
        "        labels.append(lbls)\n",
        "\n",
        "# --- Extract Xception Features ---\n",
        "xception_features = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, _ in xception_loader:\n",
        "        images = images.to(device)\n",
        "        feats = xception(images)\n",
        "        xception_features.append(feats.cpu())\n",
        "\n",
        "# --- Save Features ---\n",
        "resnet_features = torch.cat(resnet_features)\n",
        "xception_features = torch.cat(xception_features)\n",
        "labels = torch.cat(labels)\n",
        "\n",
        "torch.save(resnet_features, f\"{save_path}/resnet_features.pt\")\n",
        "torch.save(xception_features, f\"{save_path}/xception_features.pt\")\n",
        "torch.save(labels, f\"{save_path}/labels.pt\")\n",
        "\n",
        "print(\"✅ Features extracted and saved successfully.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:05:34.37128Z",
          "iopub.execute_input": "2025-04-08T08:05:34.371618Z",
          "iopub.status.idle": "2025-04-08T08:05:35.973305Z",
          "shell.execute_reply.started": "2025-04-08T08:05:34.371595Z",
          "shell.execute_reply": "2025-04-08T08:05:35.972515Z"
        },
        "id": "EWfed0vn8nNj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load features\n",
        "resnet_feats = torch.load(\"/kaggle/working/features/resnet_features.pt\")\n",
        "xception_feats = torch.load(\"/kaggle/working/features/xception_features.pt\")\n",
        "labels = torch.load(\"/kaggle/working/features/labels.pt\")\n",
        "\n",
        "# Combine features\n",
        "combined_features = torch.cat((resnet_feats, xception_feats), dim=1).numpy()\n",
        "y = labels.numpy()\n",
        "\n",
        "# Train-Test Split (for demonstration, you can use sklearn.model_selection.train_test_split)\n",
        "X_train, X_test = combined_features[:8], combined_features[8:]\n",
        "y_train, y_test = y[:8], y[8:]\n",
        "\n",
        "# Train Meta Classifier (Logistic Regression)\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"✅ Meta-classifier Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Real\", \"Fake\"])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Ensemble Output\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:06:40.723599Z",
          "iopub.execute_input": "2025-04-08T08:06:40.723914Z",
          "iopub.status.idle": "2025-04-08T08:06:41.222941Z",
          "shell.execute_reply.started": "2025-04-08T08:06:40.723889Z",
          "shell.execute_reply": "2025-04-08T08:06:41.221343Z"
        },
        "id": "iEMSzjKL8nNk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split features and labels properly\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    combined_features, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:07:53.654175Z",
          "iopub.execute_input": "2025-04-08T08:07:53.654497Z",
          "iopub.status.idle": "2025-04-08T08:07:53.660694Z",
          "shell.execute_reply.started": "2025-04-08T08:07:53.654473Z",
          "shell.execute_reply": "2025-04-08T08:07:53.659888Z"
        },
        "id": "z2MQz18D8nNk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train class counts:\", {i: list(y_train).count(i) for i in set(y_train)})\n",
        "print(\"Test class counts:\", {i: list(y_test).count(i) for i in set(y_test)})\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:08:08.531507Z",
          "iopub.execute_input": "2025-04-08T08:08:08.531829Z",
          "iopub.status.idle": "2025-04-08T08:08:08.539745Z",
          "shell.execute_reply.started": "2025-04-08T08:08:08.531804Z",
          "shell.execute_reply": "2025-04-08T08:08:08.536696Z"
        },
        "id": "36KJ7XXC8nNk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train Meta Classifier\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"✅ Ensemble (LogReg) Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\"])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Ensemble Output\")\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:09:29.634733Z",
          "iopub.execute_input": "2025-04-08T08:09:29.635066Z",
          "iopub.status.idle": "2025-04-08T08:09:29.894221Z",
          "shell.execute_reply.started": "2025-04-08T08:09:29.635041Z",
          "shell.execute_reply": "2025-04-08T08:09:29.893362Z"
        },
        "id": "X3UrhX3w8nNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"🔎 Precision: {precision:.2f}\")\n",
        "print(f\"🔄 Recall:    {recall:.2f}\")\n",
        "print(f\"🎯 F1 Score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:11:43.585087Z",
          "iopub.execute_input": "2025-04-08T08:11:43.585443Z",
          "iopub.status.idle": "2025-04-08T08:11:43.598763Z",
          "shell.execute_reply.started": "2025-04-08T08:11:43.585418Z",
          "shell.execute_reply": "2025-04-08T08:11:43.597876Z"
        },
        "id": "sPbYt20E8nNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from retinaface import RetinaFace\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "REAL_VIDEOS = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\n",
        "FAKE_VIDEOS = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n",
        "SAVE_PATH = \"/kaggle/working/lstm_features\"\n",
        "\n",
        "# Settings\n",
        "FRAME_SKIP = 30\n",
        "MAX_FRAMES = 5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create output dirs\n",
        "os.makedirs(f\"{SAVE_PATH}/real\", exist_ok=True)\n",
        "os.makedirs(f\"{SAVE_PATH}/fake\", exist_ok=True)\n",
        "\n",
        "# Transforms\n",
        "resnet_tf = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "xception_tf = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Load ResNet\n",
        "resnet = models.resnet50(weights=None)\n",
        "resnet.fc = nn.Linear(2048, 2)\n",
        "resnet.load_state_dict(torch.load(\"/kaggle/working/resnet_model.pth\"))\n",
        "resnet.fc = nn.Identity()\n",
        "resnet = resnet.to(DEVICE).eval()\n",
        "\n",
        "# Load Xception\n",
        "xception = pretrainedmodels.__dict__['xception'](pretrained=None)\n",
        "xception.last_linear = nn.Linear(xception.last_linear.in_features, 2)\n",
        "xception.load_state_dict(torch.load(\"/kaggle/working/xception_model.pth\"))\n",
        "xception.last_linear = nn.Identity()\n",
        "xception = xception.to(DEVICE).eval()\n",
        "\n",
        "# Feature extractor per frame\n",
        "def get_frame_features(face_img):\n",
        "    # Resize for each model\n",
        "    resnet_img = resnet_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    xception_img = xception_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        resnet_feat = resnet(resnet_img)\n",
        "        xception_feat = xception(xception_img)\n",
        "\n",
        "    return torch.cat((resnet_feat.cpu(), xception_feat.cpu()), dim=1)  # shape: [1, 4096]\n",
        "\n",
        "# Video → [T, 4096] feature sequence\n",
        "def process_video(video_path, save_dir, vid_name):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_idx = 0\n",
        "    sequence = []\n",
        "\n",
        "    while cap.isOpened() and len(sequence) < MAX_FRAMES:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % FRAME_SKIP == 0:\n",
        "            try:\n",
        "                faces = RetinaFace.extract_faces(frame, align=True)\n",
        "                if faces:\n",
        "                    face = faces[0]\n",
        "                    face_img = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "                    feature_vec = get_frame_features(face_img)\n",
        "                    sequence.append(feature_vec.squeeze(0))\n",
        "            except:\n",
        "                pass\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    if sequence:\n",
        "        sequence_tensor = torch.stack(sequence)  # shape: [T, 4096]\n",
        "        torch.save(sequence_tensor, f\"{save_dir}/{vid_name}.pt\")\n",
        "\n",
        "# Process real + fake videos\n",
        "for label, path in zip(['real', 'fake'], [REAL_VIDEOS, FAKE_VIDEOS]):\n",
        "    print(f\"\\n📦 Processing {label.upper()} videos:\")\n",
        "    videos = sorted(os.listdir(path))[:100]  # Start with 10 for now\n",
        "    for vid in tqdm(videos):\n",
        "        video_path = os.path.join(path, vid)\n",
        "        name = os.path.splitext(vid)[0]\n",
        "        save_dir = f\"{SAVE_PATH}/{label}\"\n",
        "        process_video(video_path, save_dir, name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:39:42.194329Z",
          "iopub.execute_input": "2025-04-08T08:39:42.194667Z",
          "iopub.status.idle": "2025-04-08T08:45:02.495996Z",
          "shell.execute_reply.started": "2025-04-08T08:39:42.194643Z",
          "shell.execute_reply": "2025-04-08T08:45:02.495021Z"
        },
        "id": "-fy2q3VX8nNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class LSTMVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for label, subfolder in enumerate(['real', 'fake']):\n",
        "            folder = os.path.join(root_dir, subfolder)\n",
        "            for file in sorted(os.listdir(folder)):\n",
        "                if file.endswith('.pt'):\n",
        "                    tensor = torch.load(os.path.join(folder, file))\n",
        "                    if tensor.size(0) == 5:  # Only keep fixed-length\n",
        "                        self.data.append(tensor)\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:45:57.489053Z",
          "iopub.execute_input": "2025-04-08T08:45:57.489366Z",
          "iopub.status.idle": "2025-04-08T08:45:57.49501Z",
          "shell.execute_reply.started": "2025-04-08T08:45:57.489338Z",
          "shell.execute_reply": "2025-04-08T08:45:57.494227Z"
        },
        "id": "XVSLy2_h8nNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DeepfakeLSTM(nn.Module):\n",
        "    def __init__(self, input_size=4096, hidden_size=256, num_layers=1):\n",
        "        super(DeepfakeLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)  # x: [batch, seq, features]\n",
        "        out = out[:, -1, :]    # take last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:46:01.025141Z",
          "iopub.execute_input": "2025-04-08T08:46:01.025454Z",
          "iopub.status.idle": "2025-04-08T08:46:01.030724Z",
          "shell.execute_reply.started": "2025-04-08T08:46:01.025427Z",
          "shell.execute_reply": "2025-04-08T08:46:01.029777Z"
        },
        "id": "iSzvvLgD8nNl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "full_dataset = LSTMVideoDataset(\"/kaggle/working/lstm_features\")\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Init model\n",
        "model = DeepfakeLSTM().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Eval\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "            output = model(x)\n",
        "            preds = output.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "            y_true.extend(y.cpu().tolist())\n",
        "            y_pred.extend(preds.cpu().tolist())\n",
        "    acc = correct / total\n",
        "    test_accuracies.append(acc)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Acc={acc:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:46:32.582726Z",
          "iopub.execute_input": "2025-04-08T08:46:32.583106Z",
          "iopub.status.idle": "2025-04-08T08:46:35.360075Z",
          "shell.execute_reply.started": "2025-04-08T08:46:32.583077Z",
          "shell.execute_reply": "2025-04-08T08:46:35.359306Z"
        },
        "id": "iU85ex5a8nNm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label=\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(test_accuracies, label=\"Accuracy\", color='green')\n",
        "plt.title(\"Test Accuracy\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:46:39.442118Z",
          "iopub.execute_input": "2025-04-08T08:46:39.442441Z",
          "iopub.status.idle": "2025-04-08T08:46:39.786392Z",
          "shell.execute_reply.started": "2025-04-08T08:46:39.442415Z",
          "shell.execute_reply": "2025-04-08T08:46:39.785613Z"
        },
        "id": "CgNoDgFZ8nNm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"🔍 LSTM Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"]))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:46:46.95715Z",
          "iopub.execute_input": "2025-04-08T08:46:46.957441Z",
          "iopub.status.idle": "2025-04-08T08:46:46.970075Z",
          "shell.execute_reply.started": "2025-04-08T08:46:46.957419Z",
          "shell.execute_reply": "2025-04-08T08:46:46.969145Z"
        },
        "id": "yav4OFOw8nNm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm --quiet\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:48:41.517689Z",
          "iopub.execute_input": "2025-04-08T08:48:41.518045Z",
          "iopub.status.idle": "2025-04-08T08:48:44.897281Z",
          "shell.execute_reply.started": "2025-04-08T08:48:41.518017Z",
          "shell.execute_reply": "2025-04-08T08:48:44.89603Z"
        },
        "id": "yVUPH8vr8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:48:56.09389Z",
          "iopub.execute_input": "2025-04-08T08:48:56.094282Z",
          "iopub.status.idle": "2025-04-08T08:48:59.046712Z",
          "shell.execute_reply.started": "2025-04-08T08:48:56.094236Z",
          "shell.execute_reply": "2025-04-08T08:48:59.046049Z"
        },
        "id": "XFGq0wtD8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import timm\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "vit.head = torch.nn.Identity()  # remove classification head\n",
        "vit = vit.to(DEVICE).eval()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:49:23.552506Z",
          "iopub.execute_input": "2025-04-08T08:49:23.552839Z",
          "iopub.status.idle": "2025-04-08T08:49:26.185417Z",
          "shell.execute_reply.started": "2025-04-08T08:49:23.552809Z",
          "shell.execute_reply": "2025-04-08T08:49:26.184726Z"
        },
        "id": "dCpos29T8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "vit_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "def get_frame_features(face_img):\n",
        "    resnet_img = resnet_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    xception_img = xception_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    vit_img = vit_transform(face_img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        resnet_feat = resnet(resnet_img)\n",
        "        xception_feat = xception(xception_img)\n",
        "        vit_feat = vit(vit_img)\n",
        "\n",
        "    return torch.cat((resnet_feat.cpu(), xception_feat.cpu(), vit_feat.cpu()), dim=1)  # [1, 6144]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:49:42.977555Z",
          "iopub.execute_input": "2025-04-08T08:49:42.977886Z",
          "iopub.status.idle": "2025-04-08T08:49:42.983926Z",
          "shell.execute_reply.started": "2025-04-08T08:49:42.977859Z",
          "shell.execute_reply": "2025-04-08T08:49:42.982831Z"
        },
        "id": "R0VBz2cM8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from retinaface import RetinaFace\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "REAL_VIDEOS = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\n",
        "FAKE_VIDEOS = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n",
        "SAVE_PATH = \"/kaggle/working/lstm_features\"\n",
        "os.makedirs(f\"{SAVE_PATH}/real\", exist_ok=True)\n",
        "os.makedirs(f\"{SAVE_PATH}/fake\", exist_ok=True)\n",
        "\n",
        "# Constants\n",
        "FRAME_SKIP = 30\n",
        "MAX_FRAMES = 5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transforms\n",
        "resnet_tf = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "xception_tf = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "vit_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Load ResNet\n",
        "resnet = models.resnet50(weights=None)\n",
        "resnet.fc = nn.Linear(2048, 2)\n",
        "resnet.load_state_dict(torch.load(\"/kaggle/working/resnet_model.pth\"))\n",
        "resnet.fc = nn.Identity()\n",
        "resnet = resnet.to(DEVICE).eval()\n",
        "\n",
        "# Load Xception\n",
        "xception = pretrainedmodels.__dict__['xception'](pretrained=None)\n",
        "xception.last_linear = nn.Linear(xception.last_linear.in_features, 2)\n",
        "xception.load_state_dict(torch.load(\"/kaggle/working/xception_model.pth\"))\n",
        "xception.last_linear = nn.Identity()\n",
        "xception = xception.to(DEVICE).eval()\n",
        "\n",
        "# Load ViT\n",
        "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "vit.head = nn.Identity()\n",
        "vit = vit.to(DEVICE).eval()\n",
        "\n",
        "# Feature extractor function\n",
        "def get_frame_features(face_img):\n",
        "    resnet_img = resnet_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    xception_img = xception_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    vit_img = vit_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        resnet_feat = resnet(resnet_img)\n",
        "        xception_feat = xception(xception_img)\n",
        "        vit_feat = vit(vit_img)\n",
        "\n",
        "    return torch.cat((resnet_feat.cpu(), xception_feat.cpu(), vit_feat.cpu()), dim=1)  # [1, 6144]\n",
        "\n",
        "# Process a single video\n",
        "def process_video(video_path, save_dir, vid_name):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_idx = 0\n",
        "    sequence = []\n",
        "\n",
        "    while cap.isOpened() and len(sequence) < MAX_FRAMES:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % FRAME_SKIP == 0:\n",
        "            try:\n",
        "                faces = RetinaFace.extract_faces(frame, align=True)\n",
        "                if faces:\n",
        "                    face = faces[0]\n",
        "                    face_img = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "                    feature_vec = get_frame_features(face_img)\n",
        "                    sequence.append(feature_vec.squeeze(0))\n",
        "            except:\n",
        "                pass\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    if sequence:\n",
        "        sequence_tensor = torch.stack(sequence)  # shape: [T, 6144]\n",
        "        torch.save(sequence_tensor, f\"{save_dir}/{vid_name}.pt\")\n",
        "\n",
        "# Process real and fake videos\n",
        "for label, path in zip(['real', 'fake'], [REAL_VIDEOS, FAKE_VIDEOS]):\n",
        "    print(f\"\\n📦 Extracting {label.upper()} videos...\")\n",
        "    videos = sorted(os.listdir(path))[:100]  # adjust count as needed\n",
        "    for vid in tqdm(videos):\n",
        "        video_path = os.path.join(path, vid)\n",
        "        name = os.path.splitext(vid)[0]\n",
        "        save_dir = f\"{SAVE_PATH}/{label}\"\n",
        "        process_video(video_path, save_dir, name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:51:35.279749Z",
          "iopub.execute_input": "2025-04-08T08:51:35.280186Z",
          "iopub.status.idle": "2025-04-08T08:57:14.712937Z",
          "shell.execute_reply.started": "2025-04-08T08:51:35.280151Z",
          "shell.execute_reply": "2025-04-08T08:57:14.712045Z"
        },
        "id": "6cl5kEzU8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class LSTMVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for label, subfolder in enumerate(['real', 'fake']):\n",
        "            folder = os.path.join(root_dir, subfolder)\n",
        "            for file in sorted(os.listdir(folder)):\n",
        "                if file.endswith('.pt'):\n",
        "                    tensor = torch.load(os.path.join(folder, file))\n",
        "                    if tensor.size(0) == 5:  # Ensure sequence length = 5\n",
        "                        self.data.append(tensor)\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:57:59.273438Z",
          "iopub.execute_input": "2025-04-08T08:57:59.273762Z",
          "iopub.status.idle": "2025-04-08T08:57:59.279923Z",
          "shell.execute_reply.started": "2025-04-08T08:57:59.273733Z",
          "shell.execute_reply": "2025-04-08T08:57:59.279158Z"
        },
        "id": "Tzi38rUA8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class LSTMVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for label, subfolder in enumerate(['real', 'fake']):\n",
        "            folder = os.path.join(root_dir, subfolder)\n",
        "            for file in sorted(os.listdir(folder)):\n",
        "                if file.endswith('.pt'):\n",
        "                    tensor = torch.load(os.path.join(folder, file))\n",
        "                    if tensor.size(0) == 5:  # Ensure sequence length = 5\n",
        "                        self.data.append(tensor)\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:58:49.828628Z",
          "iopub.execute_input": "2025-04-08T08:58:49.82901Z",
          "iopub.status.idle": "2025-04-08T08:58:49.835123Z",
          "shell.execute_reply.started": "2025-04-08T08:58:49.828947Z",
          "shell.execute_reply": "2025-04-08T08:58:49.834256Z"
        },
        "id": "qcpxhSGt8nNn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DeepfakeLSTM(nn.Module):\n",
        "    def __init__(self, input_size=6144, hidden_size=256, num_layers=1):\n",
        "        super(DeepfakeLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]  # last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:59:33.254319Z",
          "iopub.execute_input": "2025-04-08T08:59:33.254627Z",
          "iopub.status.idle": "2025-04-08T08:59:33.260178Z",
          "shell.execute_reply.started": "2025-04-08T08:59:33.254603Z",
          "shell.execute_reply": "2025-04-08T08:59:33.259136Z"
        },
        "id": "PcuFGtlQ8nNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "dataset = LSTMVideoDataset(\"/kaggle/working/lstm_features\")\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=4)\n",
        "\n",
        "# Initialize model\n",
        "model = DeepfakeLSTM().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Train\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    y_true.clear()\n",
        "    y_pred.clear()\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "            output = model(x)\n",
        "            preds = output.argmax(dim=1)\n",
        "            y_true.extend(y.cpu().tolist())\n",
        "            y_pred.extend(preds.cpu().tolist())\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = correct / total\n",
        "    test_accuracies.append(acc)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Test Acc={acc:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T08:59:37.024849Z",
          "iopub.execute_input": "2025-04-08T08:59:37.025193Z",
          "iopub.status.idle": "2025-04-08T08:59:37.185209Z",
          "shell.execute_reply.started": "2025-04-08T08:59:37.025164Z",
          "shell.execute_reply": "2025-04-08T08:59:37.18408Z"
        },
        "id": "LXdqwKob8nNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "return torch.cat((resnet_feat.cpu(), xception_feat.cpu(), vit_feat.cpu()), dim=1)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:00:37.377555Z",
          "iopub.execute_input": "2025-04-08T09:00:37.377894Z",
          "iopub.status.idle": "2025-04-08T09:00:37.383623Z",
          "shell.execute_reply.started": "2025-04-08T09:00:37.377871Z",
          "shell.execute_reply": "2025-04-08T09:00:37.382431Z"
        },
        "id": "_9TQeRZU8nNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "vit.head = torch.nn.Sequential(\n",
        "    nn.Linear(vit.head.in_features, 2048),\n",
        "    nn.ReLU()\n",
        ")\n",
        "vit = vit.to(DEVICE).eval()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:03:01.191338Z",
          "iopub.execute_input": "2025-04-08T09:03:01.191626Z",
          "iopub.status.idle": "2025-04-08T09:03:02.747939Z",
          "shell.execute_reply.started": "2025-04-08T09:03:01.191605Z",
          "shell.execute_reply": "2025-04-08T09:03:02.747039Z"
        },
        "id": "6UW502ll8nNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_frame_features(face_img):\n",
        "    resnet_img = resnet_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    xception_img = xception_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    vit_img = vit_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        resnet_feat = resnet(resnet_img)            # [1, 2048]\n",
        "        xception_feat = xception(xception_img)      # [1, 2048]\n",
        "        vit_feat = vit(vit_img)                     # [1, 2048] — now fixed!\n",
        "\n",
        "    return torch.cat((resnet_feat.cpu(), xception_feat.cpu(), vit_feat.cpu()), dim=1)  # [1, 6144]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:04:05.879126Z",
          "iopub.execute_input": "2025-04-08T09:04:05.879458Z",
          "iopub.status.idle": "2025-04-08T09:04:05.884616Z",
          "shell.execute_reply.started": "2025-04-08T09:04:05.87943Z",
          "shell.execute_reply": "2025-04-08T09:04:05.883792Z"
        },
        "id": "6IfMvVpV8nNo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import pretrainedmodels\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from retinaface import RetinaFace\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "REAL_VIDEOS = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\n",
        "FAKE_VIDEOS = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n",
        "SAVE_PATH = \"/kaggle/working/lstm_features\"\n",
        "os.makedirs(f\"{SAVE_PATH}/real\", exist_ok=True)\n",
        "os.makedirs(f\"{SAVE_PATH}/fake\", exist_ok=True)\n",
        "\n",
        "# Constants\n",
        "FRAME_SKIP = 30\n",
        "MAX_FRAMES = 5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transforms\n",
        "resnet_tf = transforms.Compose([\n",
        "    transforms.Resize((160, 160)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "xception_tf = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "vit_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Load ResNet\n",
        "resnet = models.resnet50(weights=None)\n",
        "resnet.fc = nn.Linear(2048, 2)\n",
        "resnet.load_state_dict(torch.load(\"/kaggle/working/resnet_model.pth\"))\n",
        "resnet.fc = nn.Identity()\n",
        "resnet = resnet.to(DEVICE).eval()\n",
        "\n",
        "# Load Xception\n",
        "xception = pretrainedmodels.__dict__['xception'](pretrained=None)\n",
        "xception.last_linear = nn.Linear(xception.last_linear.in_features, 2)\n",
        "xception.load_state_dict(torch.load(\"/kaggle/working/xception_model.pth\"))\n",
        "xception.last_linear = nn.Identity()\n",
        "xception = xception.to(DEVICE).eval()\n",
        "\n",
        "# Load ViT (with projection)\n",
        "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
        "vit.head = nn.Sequential(\n",
        "    nn.Linear(vit.head.in_features, 2048),\n",
        "    nn.ReLU()\n",
        ")\n",
        "vit = vit.to(DEVICE).eval()\n",
        "\n",
        "# Feature extractor function\n",
        "def get_frame_features(face_img):\n",
        "    resnet_img = resnet_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    xception_img = xception_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "    vit_img = vit_tf(face_img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        resnet_feat = resnet(resnet_img)\n",
        "        xception_feat = xception(xception_img)\n",
        "        vit_feat = vit(vit_img)\n",
        "\n",
        "    return torch.cat((resnet_feat.cpu(), xception_feat.cpu(), vit_feat.cpu()), dim=1)  # [1, 6144]\n",
        "\n",
        "# Process a single video\n",
        "def process_video(video_path, save_dir, vid_name):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_idx = 0\n",
        "    sequence = []\n",
        "\n",
        "    while cap.isOpened() and len(sequence) < MAX_FRAMES:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_idx % FRAME_SKIP == 0:\n",
        "            try:\n",
        "                faces = RetinaFace.extract_faces(frame, align=True)\n",
        "                if faces:\n",
        "                    face = faces[0]\n",
        "                    face_img = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "                    feature_vec = get_frame_features(face_img)\n",
        "                    sequence.append(feature_vec.squeeze(0))\n",
        "            except:\n",
        "                pass\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    if sequence:\n",
        "        sequence_tensor = torch.stack(sequence)  # shape: [T, 6144]\n",
        "        torch.save(sequence_tensor, f\"{save_dir}/{vid_name}.pt\")\n",
        "\n",
        "# Run for real and fake videos\n",
        "for label, path in zip(['real', 'fake'], [REAL_VIDEOS, FAKE_VIDEOS]):\n",
        "    print(f\"\\n📦 Extracting {label.upper()} videos...\")\n",
        "    videos = sorted(os.listdir(path))[:100]  # change to desired count\n",
        "    for vid in tqdm(videos):\n",
        "        video_path = os.path.join(path, vid)\n",
        "        name = os.path.splitext(vid)[0]\n",
        "        save_dir = f\"{SAVE_PATH}/{label}\"\n",
        "        process_video(video_path, save_dir, name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:08:52.696335Z",
          "iopub.execute_input": "2025-04-08T09:08:52.696642Z",
          "iopub.status.idle": "2025-04-08T09:14:20.099488Z",
          "shell.execute_reply.started": "2025-04-08T09:08:52.69662Z",
          "shell.execute_reply": "2025-04-08T09:14:20.098771Z"
        },
        "id": "GPYPwCun8nNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class LSTMVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        for label, subfolder in enumerate(['real', 'fake']):\n",
        "            folder = os.path.join(root_dir, subfolder)\n",
        "            for file in sorted(os.listdir(folder)):\n",
        "                if file.endswith('.pt'):\n",
        "                    tensor = torch.load(os.path.join(folder, file))\n",
        "                    if tensor.size(0) == 5:  # ensure 5 frames\n",
        "                        self.data.append(tensor)\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:20:48.057404Z",
          "iopub.execute_input": "2025-04-08T09:20:48.057831Z",
          "iopub.status.idle": "2025-04-08T09:20:48.065824Z",
          "shell.execute_reply.started": "2025-04-08T09:20:48.05779Z",
          "shell.execute_reply": "2025-04-08T09:20:48.064743Z"
        },
        "id": "LVBo6gDE8nNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DeepfakeLSTM(nn.Module):\n",
        "    def __init__(self, input_size=6144, hidden_size=256, num_layers=1):\n",
        "        super(DeepfakeLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)  # x: [B, T, 6144]\n",
        "        out = out[:, -1, :]    # last frame output\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:21:00.404502Z",
          "iopub.execute_input": "2025-04-08T09:21:00.404783Z",
          "iopub.status.idle": "2025-04-08T09:21:00.410413Z",
          "shell.execute_reply.started": "2025-04-08T09:21:00.404761Z",
          "shell.execute_reply": "2025-04-08T09:21:00.409433Z"
        },
        "id": "Ejwn_NNm8nNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "dataset = LSTMVideoDataset(\"/kaggle/working/lstm_features\")\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=4)\n",
        "\n",
        "# Model\n",
        "model = DeepfakeLSTM(input_size=6144).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    y_true.clear()\n",
        "    y_pred.clear()\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(DEVICE), torch.tensor(y).to(DEVICE)\n",
        "            output = model(x)\n",
        "            preds = output.argmax(dim=1)\n",
        "            y_true.extend(y.cpu().tolist())\n",
        "            y_pred.extend(preds.cpu().tolist())\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    acc = correct / total\n",
        "    test_accuracies.append(acc)\n",
        "    print(f\"📊 Epoch {epoch+1}: Train Loss = {train_losses[-1]:.4f} | Test Acc = {acc:.2%}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:21:32.072099Z",
          "iopub.execute_input": "2025-04-08T09:21:32.072446Z",
          "iopub.status.idle": "2025-04-08T09:21:34.769375Z",
          "shell.execute_reply.started": "2025-04-08T09:21:32.072415Z",
          "shell.execute_reply": "2025-04-08T09:21:34.768617Z"
        },
        "id": "oJLQXnDn8nNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.title(\"LSTM Training Progress\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:21:53.19562Z",
          "iopub.execute_input": "2025-04-08T09:21:53.195938Z",
          "iopub.status.idle": "2025-04-08T09:21:53.393487Z",
          "shell.execute_reply.started": "2025-04-08T09:21:53.195911Z",
          "shell.execute_reply": "2025-04-08T09:21:53.392647Z"
        },
        "id": "NiL3Y4228nNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"🧠 LSTM + ViT Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"]))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:22:19.57159Z",
          "iopub.execute_input": "2025-04-08T09:22:19.571929Z",
          "iopub.status.idle": "2025-04-08T09:22:19.586141Z",
          "shell.execute_reply.started": "2025-04-08T09:22:19.571899Z",
          "shell.execute_reply": "2025-04-08T09:22:19.58539Z"
        },
        "id": "3km7zeWS8nNp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"📋 LSTM + ViT Classification Report:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"]))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:29:46.932501Z",
          "iopub.execute_input": "2025-04-08T09:29:46.932831Z",
          "iopub.status.idle": "2025-04-08T09:29:46.946496Z",
          "shell.execute_reply.started": "2025-04-08T09:29:46.932806Z",
          "shell.execute_reply": "2025-04-08T09:29:46.945556Z"
        },
        "id": "oUPE6feF8nNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - LSTM + ViT\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:30:15.503562Z",
          "iopub.execute_input": "2025-04-08T09:30:15.503935Z",
          "iopub.status.idle": "2025-04-08T09:30:16.340155Z",
          "shell.execute_reply.started": "2025-04-08T09:30:15.503901Z",
          "shell.execute_reply": "2025-04-08T09:30:16.339347Z"
        },
        "id": "4E50z2Wm8nNt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.title(\"Training Progress\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:30:39.53692Z",
          "iopub.execute_input": "2025-04-08T09:30:39.537637Z",
          "iopub.status.idle": "2025-04-08T09:30:39.725927Z",
          "shell.execute_reply.started": "2025-04-08T09:30:39.537602Z",
          "shell.execute_reply": "2025-04-08T09:30:39.725132Z"
        },
        "id": "Q7PDov4w8nNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ✅ Accuracy\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\n🎯 Final Accuracy (ResNet + Xception + ViT + LSTM): {acc * 100:.2f}%\\n\")\n",
        "\n",
        "# ✅ Precision, Recall, F1-Score\n",
        "print(\"📋 Classification Report (ResNet + Xcep + ViT + LSTM):\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"]))\n",
        "\n",
        "# ✅ Optional: Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix - Hybrid Ensemble\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T09:33:55.232439Z",
          "iopub.execute_input": "2025-04-08T09:33:55.232772Z",
          "iopub.status.idle": "2025-04-08T09:33:55.432522Z",
          "shell.execute_reply.started": "2025-04-08T09:33:55.232751Z",
          "shell.execute_reply": "2025-04-08T09:33:55.431791Z"
        },
        "id": "RyAnsH0W8nNu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ibMk9phG8nNu"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}